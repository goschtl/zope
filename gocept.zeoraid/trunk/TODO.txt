====
TODO
====

1.0
===

Stabilization
-------------

- Check edge cases for locking on all methods so that degrading a storage
  works under all circumstances.

- The second pass of the recovery isn't thread safe. Ensure that only one
  recovery can run at a time. (This is probably a good idea anyway because of
  IO load.)

- Run some manual tests for weird situations, high load, ...

- Compatibility to which ZODB clients and which ZEO servers? Best would be to
  support Zope 2.9 and Zope 3.4.

- Re-check API usage and definition for ZODB 3.8 as our base.

- Ensure that blob-caching parameters are equal for all clientstorages

- Provide RAID-aware blob storage implementation that ignores requests on a
  shared file system that were handled already and are consistent.

- Disallow packing while a storage is recovering.

- Disallow recovering multiple storages in parallel.

- Manager client: setup correctly so that invalidations do not trigger errors

- Manager client: provide current recovery message for a storage that is
  recovering

- Allow ZEORaid to startup `quickly` even when a backend zeo server is not
  available (thread-parallelize the opening of storages?)

- The exception branch for ClientDisconnected (StorageError) is not tested
  during unit/functional tests (__apply_storage)

- Pack may never be run while a storage is recovering.

- Windows support

Feature-completeness
--------------------

- Recovery with blob support.

- Support undo.

Blob support
-------------

- Create flag `shared-blob-dir` for RAIDStorage
- Functionality for shared blob-dir
- Functionality for unshared blob-dir
  - clean up unconsumed hard links
  - clean up temporary directories

Cleanup
-------

- Offer the `read only` option on RAIDStorage through ZConfig schema.

- Remove print statements and provide logging.

Future
======

- Allow asynchronous backend storages (e.g. for off-site replication)

- Make write access to backend storages parallel (for better write
  performance).

- Balance read requests over varying backends to optimize caching and
  distribute IO load. (Beware of the hard-coded priority queue that we use for
  packing.)

- Allow online reconfiguration of the RAID storage (e.g. for adding or
  removing new backend servers).

- Document how to make the RAID server itself reliable (leveraging the
  statelessness of ZEORaid and a hot standby).

- Verify parallel/backend invalidations + optimize invalidations that they get
  passed on only once.

- Guarantee a recovery rate larger than the rate of new commits (one idea is
  to use a "credit point system").
